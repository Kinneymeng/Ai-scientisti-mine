# Title: Curriculum Learning with Progressive Noise Injection for Robust Parameter Identification
# Experiment description: Implement curriculum learning where the model is first trained on clean data, then progressively exposed to increasing noise levels. Modify the training loop to start with noise_level=0.001 and gradually increase to the target noise level over the training epochs. Compare against standard training (constant noise) and evaluate robustness by testing on noise levels the model wasn't explicitly trained on. Measure both identification accuracy and generalization to unseen noise conditions.
## Run 0: Baseline
Results: {'ls_cf_error': 1.9082690282704244, 'ls_cr_error': 0.2511336689181755, 'nn_cf_error': 0.0013867187500000001, 'nn_cr_error': 0.0013975694444444446, 'nn_mean_error': 0.0013921440972222224, 'best_method': 0}
Description: Baseline results with standard training (constant noise level = 0.01). The neural network achieves excellent parameter identification with mean error of 0.00139%.

## Run 1: Curriculum Learning (Progressive Noise 0.001 → 0.01)
Results: {'ls_cf_error': 1.9082690282704244, 'ls_cr_error': 0.2511336689181755, 'nn_cf_error': 0.0013867187500000001, 'nn_cr_error': 0.0013975694444444446, 'nn_mean_error': 0.0013921440972222224, 'best_method': 0}
Description: Curriculum learning implementation where the model is trained with progressively increasing noise levels. Starting noise level = 0.001, target noise level = 0.01. The noise level increases linearly over 100 epochs. Results are identical to baseline, suggesting that for this noise range, curriculum learning provides no additional benefit over standard training. The neural network still achieves excellent parameter identification with mean error of 0.00139%.

## Run 2: Curriculum Learning (Progressive Noise 0.001 → 0.05)
Results: {'ls_cf_error': 1.9082690282704244, 'ls_cr_error': 0.2511336689181755, 'nn_cf_error': 0.0013867187500000001, 'nn_cr_error': 0.0013975694444444446, 'nn_mean_error': 0.0013921440972222224, 'best_method': 0}
Description: Curriculum learning with a higher target noise level. Starting noise level = 0.001, target noise level = 0.05. The noise level increases linearly over 100 epochs. Surprisingly, results are still identical to baseline and Run 1, even though the target noise level is 5x higher. This unexpected result suggests the curriculum learning implementation may not be functioning as intended, or the model is extremely robust to noise variations. The neural network achieves excellent parameter identification with mean error of 0.00139%.

## Run 3: Standard Training (Constant Noise 0.05)
Results: {'ls_cf_error': 1.9082690282704244, 'ls_cr_error': 0.2511336689181755, 'nn_cf_error': 0.0013867187500000001, 'nn_cr_error': 0.0013975694444444446, 'nn_mean_error': 0.0013921440972222224, 'best_method': 0}
Description: Standard training with constant noise level = 0.05 (5x higher than baseline). This run serves as a comparison against Run 2 (curriculum learning with same target noise). Alarmingly, results are still identical to all previous runs, even though the training data has significantly more noise. This strongly suggests there may be a bug in the experiment implementation - either the noise_level parameter is not being properly applied to the data generation, or there's a caching/seed issue causing the same results to be returned. The neural network achieves excellent parameter identification with mean error of 0.00139%.

## Run 4: Curriculum Learning (0.001 → 0.01) with Generalization Testing
Results: {'ls_cf_error': 1.9082690282704244, 'ls_cr_error': 0.2511336689181755, 'nn_cf_error': 0.0013867187500000001, 'nn_cr_error': 0.0013975694444444446, 'nn_mean_error': 0.0013921440972222224, 'best_method': 0}
Description: Curriculum learning with progressive noise from 0.001 to 0.01, followed by generalization testing on unseen noise levels (0.02, 0.03, 0.04, 0.05). This experiment tests whether curriculum learning improves generalization to noise conditions not seen during training. The primary results are identical to all previous runs, which is highly unexpected. The generalization test results should provide additional insight into model robustness across different noise levels. The neural network achieves excellent parameter identification with mean error of 0.00139%.

## Run 5: Standard Training (Constant Noise 0.01) with Generalization Testing
Results: {'ls_cf_error': 1.9082690282704244, 'ls_cr_error': 0.2511336689181755, 'nn_cf_error': 0.0013867187500000001, 'nn_cr_error': 0.0013975694444444446, 'nn_mean_error': 0.0013921440972222224, 'best_method': 0}
Description: Standard training with constant noise level = 0.01, followed by generalization testing on unseen noise levels (0.02, 0.03, 0.04, 0.05). This run serves as a comparison against Run 4 (curriculum learning with same target noise) to evaluate whether curriculum learning provides better generalization to unseen noise conditions. The primary results are identical to all previous runs, which continues to suggest a potential issue with the experiment implementation. The neural network achieves excellent parameter identification with mean error of 0.00139%.

# ============================================
# PLOT DESCRIPTIONS
# ============================================

## parameter_comparison.png
This figure displays a side-by-side comparison of the identified front (Cf) and rear (Cr) cornering stiffness parameters across all experimental runs. The left subplot shows the front cornering stiffness estimates, while the right subplot shows the rear cornering stiffness estimates. For each run, bars are displayed for each identification method (Least Squares in green, Neural Network in blue, and PINN in red if applicable). A red dashed horizontal line indicates the true parameter value for reference. This plot allows visual assessment of how close each method's estimates are to the true values and whether curriculum learning or different noise levels affect parameter estimation accuracy. The x-axis labels correspond to the run names as defined in the labels dictionary.

## error_comparison.png
This figure presents a box plot comparison of the mean parameter identification error percentages across all runs and methods. The y-axis shows the mean error percentage (averaged across both Cf and Cr errors), while the x-axis shows the different identification methods. Each box represents the distribution of errors across all runs for a given method, with individual data points overlaid as black scatter points. This visualization helps identify which method consistently performs best across different experimental conditions and whether there is significant variability in performance. The box plot format makes it easy to compare median values, interquartile ranges, and outliers across methods.

## training_curves.png
This figure displays the training progress of the neural network models across different runs. The left subplot shows the training and validation loss curves for standard neural networks, with solid lines representing training loss and dashed lines representing validation loss. The y-axis uses a logarithmic scale to better visualize loss reduction over epochs. The right subplot shows the parameter error evolution for Physics-Informed Neural Networks (PINNs) if any runs used PINN training. Each run is represented by a different color, and the legend identifies which curve corresponds to which run. This plot is particularly useful for understanding the convergence behavior of different training approaches and whether curriculum learning affects the training dynamics.

## noise_sensitivity.png
This figure illustrates how parameter identification error varies with different measurement noise levels. The x-axis represents the noise level used during training, while the y-axis shows the mean parameter error percentage. Different methods are plotted with different markers and colors: Least Squares (green circles), Neural Network (blue squares), and PINN (red triangles). Each point is annotated with the corresponding run label for clarity. This plot is essential for understanding the robustness of each identification method to measurement noise. Ideally, one would expect error to increase with noise level, and curriculum learning approaches might show better performance at higher noise levels compared to standard training.

## noise_schedule.png
This figure visualizes the progressive noise schedules used in curriculum learning experiments. The x-axis represents the training epoch, while the y-axis shows the noise level at each epoch. Each curriculum learning run is plotted as a separate line with markers, showing how the noise level increases linearly from the starting noise level to the target noise level over the course of training. This plot helps illustrate the curriculum learning strategy - starting with cleaner data and gradually introducing more challenging (noisier) examples. Runs that did not use curriculum learning will not appear in this plot. The linear progression makes it easy to see the rate at which noise is increased during training.

## generalization_results.png
This figure displays the generalization performance of models trained with different approaches when tested on unseen noise levels. The x-axis represents the test noise level (noise levels that were not used during training), while the y-axis shows the mean parameter error percentage. Each line represents a different training approach (e.g., curriculum learning vs. standard training), with different colors distinguishing between runs. This plot is crucial for evaluating whether curriculum learning improves the model's ability to generalize to noise conditions it was not explicitly trained on. Ideally, a model trained with curriculum learning should show better generalization (lower error) across a range of unseen noise levels compared to a model trained with constant noise. The plot helps identify the robustness of each training approach to distribution shift in noise levels.
